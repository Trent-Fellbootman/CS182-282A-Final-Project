{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Cycle Generative Adversarial Network (CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/6q_wnl392rn1_6g2d_l2hnrh0000gr/T/ipykernel_68291/1337210060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualizations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_pairs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_point_correspondence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/DTU/Kandidat - Business Analytics/3. Semester/COMPSCI 282A-999 Designing, Visualizing and Understanding Deep Neural Networks/Project/CS182-282A-Final-Project/library/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvanilla_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/DTU/Kandidat - Business Analytics/3. Semester/COMPSCI 282A-999 Designing, Visualizing and Understanding Deep Neural Networks/Project/CS182-282A-Final-Project/library/models/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flax'"
     ]
    }
   ],
   "source": [
    "# Load the necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from library import datasets, models\n",
    "from library.visualizations.point_pairs import visualize_point_correspondence\n",
    "from flax import linen as nn\n",
    "from jax import random, numpy as jnp\n",
    "import optax\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, you saw how GAN tries to learn to generate samples from a desired distribution. In this notebook, you will use CycleGAN to learn how to map samples from one distribution to another. One of the key advantages of CycleGAN is that it does not require a paired dataset. Instead, it learns the sample-to-sample correspondence between two datasets by enforcing cycle consistency. In this homework, you will consider the mapping between two distributions of the blob dataset you saw in the GAN notebook. These are illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(5)\n",
    "key_A, key_B = random.split(key)\n",
    "\n",
    "# Create dataset\n",
    "A = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1, key=key_A)\n",
    "B = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1, key=key_B)\n",
    "\n",
    "real_A = A.get_tensors()\n",
    "real_B = B.get_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"), name='Real B'), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (A): Initialize CycleGAN and interpret generated fake samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you already learned how to create the discriminator and generator in the GAN notebook, you can reuse their architectures below. Remember that we have two different distributions now instead of one for GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambient dimension, discriminator and generator\n",
    "AMBIENT_DIM = 2\n",
    "\n",
    "discriminator_A = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(1),\n",
    "])\n",
    "\n",
    "discriminator_B = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(1),\n",
    "])\n",
    "\n",
    "generator_AB = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(AMBIENT_DIM),\n",
    "])\n",
    "\n",
    "generator_BA = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(AMBIENT_DIM),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with generators, discriminators and loss function.\n",
    "modules = {\n",
    "        'generator_AB': generator_AB,\n",
    "        'generator_BA': generator_BA,\n",
    "        'discriminator_A': discriminator_A,\n",
    "        'discriminator_B': discriminator_B,\n",
    "    }\n",
    "\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generators from untrained model and draw samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from untrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Are the generated fake samples as you expected? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (B): Implement cycle consistency loss and train CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CycleGAN model, we aim to solve the minimax game between the generators and the discriminators:\n",
    "$$\\underset{G_{AB}, G_{BA}}{min}\\: \\underset{D_A, D_B}{max}\\: V(G_{AB}, G_{BA}, D_A, D_B) = \\mathcal{L}(G_{AB},G_{BA},D_A, D_B)$$\n",
    "\n",
    "The full objective is defined as:\n",
    "$$\\mathcal{L}(G_{AB},G_{BA},D_A, D_B) = \\mathcal{L}(G_{AB},D_B,A,B) + \\mathcal{L}(G_{BA},D_A,B,A) +\\lambda \\mathcal{L}_{cyc}(G_{AB}, G_{BA})$$\n",
    "where $\\lambda$ controls the relative importance of the two objectives (i.e. GAN or cycle consistency loss) and where the GAN and cycle consistency losses are defined as:\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{GAN}(G_{AB}, D_B, A, B) &= \\mathbb{E}_{b \\sim p_{data}(b)}\\left[log \\: D_B(b)\\right]+\\mathbb{E}_{a \\sim p_{data}(a)}\\left[log(1-D_B(G_{AB}(a)))\\right] \\\\\n",
    "\\mathcal{L}_{cyc}(G_{AB}, G_{BA}) &= \\mathbb{E}_{a \\sim p_{data}(a)}\\left[||G_{BA}(G_{AB}(a))-a||_1\\right] + \\mathbb{E}_{b \\sim p_{data}(b)}\\left[||G_{AB}(G_{BA}(b))-b||_1\\right]\n",
    "\\end{align*}\n",
    "Note that $\\mathcal{L}(G_{BA},D_A,B,A)$ is calculated in similar fashion as $\\mathcal{L}(G_{AB},D_B,A,B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this part, you should implement the GAN and cycle consistency loss functions from above in the train method in `cyclegan.py`. Once you have successfully implemented the functions, you can train the CycleGAN model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CycleGAN\n",
    "lambda_ = 5\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=1000, cycle_loss_weight=lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (C): CycleGAN loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "for key, value in history.items():\n",
    "    plt.plot(range(len(value)), value, label=key)\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves of generators and discriminators\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Looking at the loss curves above, can you identify a limitation of CycleGAN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.generator(real_B)\n",
    "fake_B = gan_AB.generator(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from trained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, visualize how the generators transforms samples across distributions.**\n",
    "\n",
    "Explanation of the graphs:\n",
    "\n",
    "The function `visualize_point_correspondence` visualizes the correspondence across two sets of points by giving points that correspond to each other the same color. The left subplot corresponds to the points passed in as the first argument, while the right subplot corresponds to the second argument. For example, if you call `visualize_point_correspondence(real_A, fake_B)`, where `fake_B` is transformed from `real_A` by the generator, then an orange point in the left subplot is transformed by the generator to the corresponding orange point in the right subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_B, fake_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (D): Visualize sample correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_B, fake_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does it seem that the clusters of ”real A samples” and the clusters of ”fake A samples” are switched, despite a low cycle loss observed at the end of training?**\n",
    "\n",
    "Answer the question in written portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the remaining cells in this question are SOLUTION ONLY. Remove them in the homework.\n",
    "recon_A = gan_BA.generator(gan_AB.generator(real_A))\n",
    "recon_B = gan_AB.generator(gan_BA.generator(real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, recon_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_B, recon_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, real_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E: Pretraining with GAN loss only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain with $\\lambda = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "lambda_pretrain = 0 # TODO: Enter lambda for pretraining\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from pretrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train with both GAN loss and cycle loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "lambda_train = 5 # TODO: Enter lambda value for fine tuning\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from the trained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How does pretraining without cycle loss affect subsequent training where cycle loss is present? Can you give a possible explanation?**\n",
    "\n",
    "Write your answers in the written portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F: Pretraining with GAN loss and cycle loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain with $\\lambda = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "lambda_pretrain = 5 # TODO: Enter lambda for pretraining\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=1000, cycle_loss_weight=lambda_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from pretrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train with GAN loss ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "lambda_train = 0 # TODO: Enter lambda value for fine tuning\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from the trained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How does pretraining with cycle loss affect subsequent training where cycle loss is absent? Can you give a possible explanation?**\n",
    "\n",
    "Write your answers in the written portion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d5bbc90547c8ba0b4e6ac5d6ff9c75838f9b373a6adaf23b1514500c20e42f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
