{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Cycle Generative Adversarial Network (CycleGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "Introduction of GANs\n",
    "- Applications\n",
    "- Introduce generator and discriminator\n",
    "- Adversarial/GAN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models has seen a huge growth in popularity the latest years due to its unique properties. The usage of these models cover applications that directly require generation, creating more training data for machine learning or data augmentation for privacy concerns. A popular choice of generative models are the Generative Adversarial Networks (GANs), which have been used to generate realistic photographs, image style transfer, face ageing in pictures and [many more impressive applications](https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/).\n",
    "\n",
    "GANs are constructed by training two models, a generator and a discriminator, that are competing against each other. The generator is trying to learn the distribution of a given dataset and is used to generate fake samples following this distribution. The discriminator is trying to determine whether a sample is from the \"fake\" distribution or the true distribution. Therefore, the generator and discriminator are considered as adversaries, since they are both trying to fool each other. The [original GAN paper](https://arxiv.org/pdf/1406.2661.pdf) provides a comical example of their roles, where the generative model is analogous to a team of counterfeiters, trying to produce fake currency and use it without detection. The discriminator is analogous to the police, which are trying to detect the counterfeit currency. The competition between these two encourages both to improve their methods until the counterfeit currency is indistinguishable from real currency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The generator and discriminator are both neural networks, whose architecture greatly depends on the task at hand. The most popular domain for GANs are with images and convolutional layers are primarily used as a result. Due\n",
    "\n",
    "- Introduction to Points dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell where generator and discriminator are constructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell with training loop. Complete blank parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mode collapse\n",
    "- What is it\n",
    "- Why does it happen\n",
    "  - KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is collapse\n",
    "\n",
    "A GAN is successfully trained when it reaches two goals:\n",
    "- The generator can reliably generates samples to fool the discriminator\n",
    "- The generator can generates samples that are as diverse as the distribution in real-world data\n",
    "\n",
    "Mode collapse means that the model fails the second goal and generate similar or even identical samples.For example, we expect our model to generate number frome 0 to 9 with MNIST dataset. However, if the results only contain 0 and miss the other numbers, we say that the model meets mode collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does it happen\n",
    "\n",
    "#### What is KL divergence\n",
    "\n",
    "To measure the differece two distributions over the same variale $x$, we can use a measurement called Kullback-Leibler divergence. Let $p(x)$ and $q(x)$ are two probablity distribution of discrete random varaible $x$. The formula of KL divergence is defined in following equation:\n",
    "\n",
    "$$D_{KL}(p(x)||q(x)) = \\int_{-\\infty}^\\infty p(x)ln\\frac{p(x)}{q(x)}dx $$\n",
    "\n",
    "#### KL divergence in GAN\n",
    "For GAN, we can use KL divergence to meansure the similarity of real data distributioin $P_r(x)$ and genereated sample distribution $P_g(x)$. The formula goes like \n",
    "\n",
    "$$D_{KL}(p_g(x)||p_r(x)) = \\int_{x} p_g(x)ln\\frac{p_g(x)}{p_r(x)}dx$$\n",
    "\n",
    "The generator will have two bad situations:\n",
    "- Generate unreal samples. For the unreal samples, $p_g(x) > 0$  and $p_r(x)\\approx 0$, so the KL divergence will be nearly $\\infty$\n",
    "- Can't generate real samples. For those real samples that can't be generated, $p_r(x) > 0$ and $p_g(x) \\approx 0$, so the KL divergence will be nearly 0\n",
    "\n",
    "GAN tries to make the real data distribution and generated samples distribution similar, therefore, it requires small KL divergence. In terms of the first situation we analysed above, it will lead to huge KL divergence, so the generator avoids generating unreal samples. As for the second situation, it gets low KL divergence, so GAN are likely to avoid generate other real samples, and end up generating samples that are similiar or identical. To conclude, the generator in GAN may thinks that generating similar samples are enought to fool the discriminator with low risk, which causes the mode collapse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO   \n",
    "explain that mode collapse happens in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain the introduction of cycle consistency loss with images\n",
    "    - How does it combat mode collapse? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell where two generator and discriminator are constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell with training loop. Implement cycle consistency loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limitations of CycleGAN\n",
    "    - Geometric translations (i.e. cat to dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('eecs282a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c45f58d2deeb0de52fe5888272cdc1cf167671d21d590a21df033350abd3640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
