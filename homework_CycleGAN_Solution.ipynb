{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Cycle Generative Adversarial Network (CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from library import datasets, models\n",
    "from library.visualizations.point_pairs import visualize_point_correspondence\n",
    "from library.utils.numerical_checking import NumericalCheckingRecord\n",
    "from library.utils.helper_functions import check_model_forward\n",
    "from flax import linen as nn, serialization\n",
    "from jax import random, numpy as jnp\n",
    "import optax\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, you saw how GAN tries to learn to generate samples from a desired distribution. In this notebook, you will use CycleGAN to learn how to map samples from one distribution to another. One of the key advantages of CycleGAN is that it does not require a paired dataset. Instead, it learns the sample-to-sample correspondence between two datasets by enforcing cycle consistency. In this homework, you will consider the mapping between two distributions of the blob dataset you saw in the GAN notebook. These are illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(5)\n",
    "key_A, key_B = random.split(key)\n",
    "\n",
    "# Create dataset\n",
    "A = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1, key=key_A)\n",
    "B = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1, key=key_B)\n",
    "\n",
    "real_A = A.get_tensors()\n",
    "real_B = B.get_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"), name='Real B'), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (A): Initialize CycleGAN and interpret generated fake samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you already learned how to create the discriminator and generator in the GAN notebook, you can reuse their architectures below. Remember that we have two different distributions now instead of one for GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambient dimension, discriminator and generator\n",
    "AMBIENT_DIM = 2\n",
    "\n",
    "discriminator_A = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(1),\n",
    "])\n",
    "\n",
    "discriminator_B = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(1),\n",
    "])\n",
    "\n",
    "generator_AB = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(AMBIENT_DIM),\n",
    "])\n",
    "\n",
    "generator_BA = nn.Sequential([ # TODO: Copy from GAN notebook\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(AMBIENT_DIM),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with generators, discriminators and loss function.\n",
    "modules = {\n",
    "        'generator_AB': generator_AB,\n",
    "        'generator_BA': generator_BA,\n",
    "        'discriminator_A': discriminator_A,\n",
    "        'discriminator_B': discriminator_B,\n",
    "    }\n",
    "\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_states('checks/untrained_cycle_gan_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generators from untrained model and draw samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation check\n",
    "expected_fake_A = NumericalCheckingRecord.load('checks/cyclegan_generatorBA_check')\n",
    "expected_fake_B = NumericalCheckingRecord.load('checks/cyclegan_generatorAB_check')\n",
    "assert expected_fake_A.data.shape == fake_A.shape, \"Ensure output shape matches what you set for GAN\"\n",
    "assert expected_fake_B.data.shape == fake_B.shape, \"Ensure output shape matches what you set for GAN\"\n",
    "assert expected_fake_A.check(fake_A), \"Remember your architecture should match what you set for GAN\"\n",
    "assert expected_fake_B.check(fake_B), \"Remember your architecture should match what you set for GAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from untrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Are the generated fake samples as you expected? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers in the written portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (B): Implement cycle consistency loss and train CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CycleGAN model, we aim to solve the minimax game between the generators and the discriminators:\n",
    "$$\\underset{G_{AB}, G_{BA}}{min}\\: \\underset{D_A, D_B}{max}\\: V(G_{AB}, G_{BA}, D_A, D_B) = \\mathcal{L}(G_{AB},G_{BA},D_A, D_B)$$\n",
    "\n",
    "The full objective is defined as:\n",
    "$$\\mathcal{L}(G_{AB},G_{BA},D_A, D_B) = \\mathcal{L}(G_{AB},D_B,A,B) + \\mathcal{L}(G_{BA},D_A,B,A) +\\lambda \\mathcal{L}_{cyc}(G_{AB}, G_{BA})$$\n",
    "where $\\lambda$ controls the relative importance of the two objectives (i.e. GAN or cycle consistency loss) and where the GAN and cycle consistency losses are defined as:\n",
    "\\begin{align*}\n",
    "\\mathcal{L}_{GAN}(G_{AB}, D_B, A, B) &= \\mathbb{E}_{b \\sim p_{data}(b)}\\left[log \\: D_B(b)\\right]+\\mathbb{E}_{a \\sim p_{data}(a)}\\left[log(1-D_B(G_{AB}(a)))\\right] \\\\\n",
    "\\mathcal{L}_{cyc}(G_{AB}, G_{BA}) &= \\mathbb{E}_{a \\sim p_{data}(a)}\\left[||G_{BA}(G_{AB}(a))-a||_1\\right] + \\mathbb{E}_{b \\sim p_{data}(b)}\\left[||G_{AB}(G_{BA}(b))-b||_1\\right]\n",
    "\\end{align*}\n",
    "Note that $\\mathcal{L}(G_{BA},D_A,B,A)$ is calculated in similar fashion as $\\mathcal{L}(G_{AB},D_B,A,B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this part, you should implement the GAN and cycle consistency loss functions from above in the train method in `cyclegan.py`. Once you have successfully implemented the functions, you can train the CycleGAN model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile_generator_grad_fn(cycle_loss_weight=1.0, cycle_loss_mask='None')\n",
    "\n",
    "model.load_states('checks/untrained_cycle_gan_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: These are code to generate checkpoints. REMOVE them in the homework notebook.\n",
    "\n",
    "# from flax import serialization\n",
    "\n",
    "# dummy_inputs_A = random.normal(random.PRNGKey(0), (100, AMBIENT_DIM))\n",
    "# dummy_inputs_B = random.normal(random.PRNGKey(1), (100, AMBIENT_DIM))\n",
    "    \n",
    "# NumericalCheckingRecord((dummy_inputs_A, dummy_inputs_B)).save('checks/dummy_inputs_cyclegan.rec')\n",
    "# NumericalCheckingRecord(model.eval_generator_grads(dummy_inputs_A, dummy_inputs_B)).save('checks/untrained_cyclegan_generator_gradients.rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs_A, dummy_inputs_B = NumericalCheckingRecord.load('checks/dummy_inputs_cyclegan.rec').data\n",
    "gradients = model.eval_generator_grads(dummy_inputs_A, dummy_inputs_B)\n",
    "\n",
    "record = NumericalCheckingRecord.load('checks/untrained_cyclegan_generator_gradients.rec')\n",
    "\n",
    "assert record.check(gradients), 'Gradient checking failed!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (C): Training CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CycleGAN\n",
    "lambda_ = 5\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=1000, cycle_loss_weight=lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "for key, value in history.items():\n",
    "    plt.plot(range(len(value)), value, label=key)\n",
    "plt.legend()\n",
    "plt.title(\"Loss curves of generators and discriminators\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Looking at the loss curves above, can you identify a limitation of CycleGAN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers in the written portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize how the generators transformed samples across distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.generator(real_B)\n",
    "fake_B = gan_AB.generator(real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from trained generators\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (D): Visualize sample correspondence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the graphs:\n",
    "\n",
    "The function `visualize_point_correspondence` visualizes the correspondence across two sets of points by giving points that correspond to each other the same color. The left subplot corresponds to the points passed in as the first argument, while the right subplot corresponds to the second argument. For example, if you call `visualize_point_correspondence(real_A, fake_B)`, where `fake_B` is transformed from `real_A` by the generator, then an orange point in the left subplot is transformed by the generator to the corresponding orange point in the right subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, fake_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_B, fake_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does it seem that the clusters of real A samples and the clusters of fake A samples are switched, despite a low cycle loss observed at the end of training?**\n",
    "\n",
    "Answer the question in written portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the remaining cells in this question are SOLUTION ONLY. Remove them in the homework.\n",
    "recon_A = gan_BA.generator(gan_AB.generator(real_A))\n",
    "recon_B = gan_AB.generator(gan_BA.generator(real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, recon_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_B, recon_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_point_correspondence(real_A, real_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (E): Pretraining with GAN loss only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, run pretraining with only GAN loss and then introduce it after 500 epochs. You do not need to implement any code for Part (E) and (F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain with $\\lambda = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "lambda_pretrain = 0\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from pretrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train with both GAN loss and cycle loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further training\n",
    "lambda_train = 5\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from the trained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How does pretraining without cycle loss affect subsequent training where cycle loss is present? Can you give a possible explanation?**\n",
    "\n",
    "Write your answers in the written portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (F): Pretraining with GAN loss and cycle consistency loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize\n",
    "model = models.cyclegan.CycleGAN(modules, (AMBIENT_DIM,), (AMBIENT_DIM,))\n",
    "model.initialize(optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain with $\\lambda = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain\n",
    "lambda_pretrain = 5\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=1000, cycle_loss_weight=lambda_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from pretrained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, train with GAN loss ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning\n",
    "lambda_train = 0 # TODO: Enter lambda value for fine tuning\n",
    "history = model.train(A, B, optax.adam(learning_rate=5e-3), print_every=5, batch_size=1000, num_epochs=500, cycle_loss_weight=lambda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain generators and samples\n",
    "gan_AB, gan_BA = model.create_distribution()\n",
    "fake_A = gan_BA.draw_samples(real_B)\n",
    "fake_B = gan_AB.draw_samples(real_A)\n",
    "\n",
    "# Visualize samples\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles = [\"Distribution A\", \"Distribution B\"])\n",
    "fig.add_trace(go.Scatter(x=real_A[:, 0], y=real_A[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real A'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=real_B[:, 0], y=real_B[:, 1], mode='markers', marker=dict(color=\"green\"),name='Real B'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=fake_A[:, 0], y=fake_A[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_B[:, 0], y=fake_B[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake', showlegend=False), row=1, col=2)\n",
    "fig.update_layout(title_text=\"Real samples and fake samples from the trained generators\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How does pretraining with cycle loss affect subsequent training where cycle loss is absent? Can you give a possible explanation?**\n",
    "\n",
    "Write your answers in the written portion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cs182project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fec1743f822f2dd43bfb656bddbefe137b7c1918b0f71f6477d5e31a257d081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
