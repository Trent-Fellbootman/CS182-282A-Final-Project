{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "c:\\Users\\Bruger\\miniconda3\\envs\\cs182project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from library import datasets, models\n",
    "from flax import linen as nn\n",
    "from jax import random, numpy as jnp\n",
    "import optax\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The generator and discriminator are both neural networks, whose architecture greatly depends on the task at hand. The most popular domain for GANs is on images, which means that ResNets and convolutions are part of the generator and discriminator architecture. Due to the computational complexity of using images and thereby training deep models, this homework will consider a points dataset, where data points are generated according a multivariate distribution with noise. As a result, the neural networks become shallower, and their architecture will deviate from what is used in the original CycleGAN paper. \n",
    "\n",
    "The points dataset in this homework is similar to the cluster dataset used in the [GAN Lab]( https://poloclub.github.io/ganlab/). GAN Lab also provides a great visualization while training a GAN. In essence, we are interested in training a GAN that can output data points within the two clusters seen in the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1)\n",
    "data = dataset.get_tensors()\n",
    "px.scatter(x=data[:, 0], y=data[:, 1], color=jnp.ones(len(data)), range_color=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (B): Define discriminator and generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, consider the discriminator and generator as multi-layer perceptrons with ReLU activations between each layer. The number of layers and neurons is a design choice, but it is important to consider the differing dimensions of the final layer of both networks. The generator should generate output of similar dimension as the data that describes the distribution you wish to learn. The discriminator should generate a single logit, indicating whether it believes some given input is from the true distribution or the generated one.\n",
    "\n",
    "Using the above, define the ouput dimensions of the discriminator and generator below. The rest of the architecture has already been given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambient dimension, discriminator and generator\n",
    "AMBIENT_DIM = 2\n",
    "\n",
    "discriminator = nn.Sequential([\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(1), # TODO: Complete output dimension\n",
    "])\n",
    "\n",
    "generator = nn.Sequential([\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(AMBIENT_DIM), # TODO: Complete output dimension\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (C): GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training can commence, you need to implement the loss functions to train the generator and discriminator. \n",
    "The discriminator is trained to maximize the probability of assigning the correct labels for both real samples and generated samples, and train the generator to fool the discriminator simultaneously. In other words, generator and discriminator are playing a minmax game with the value function $V(G, D)$:\n",
    "\n",
    "$$\\underset{G}{min}\\:\\underset{D}{max} \\:V(G, D) = \\mathbb {E}_{x\\sim p_{data}(x)}[log\\:D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[log(1 - D(G(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective can decomposed into an individual loss function for the discriminator and generator.\n",
    "\n",
    "The discriminator *D* tries to maximize the following function, which is largely similar to binary cross entropy\n",
    "$$\\nabla_{\\theta_d} \\frac{1}{m}\\sum_{i=1}^m \\left[log\\: D\\left(\\mathbb{x}^{(i)}\\right)+log \\: \\left(1-D\\left(G\\left(z^{(i)}\\right)\\right)\\right)\\right]$$\n",
    "where *m* is the number of examples in the minibatch, *x* is the true samples, *z* is a vector of noise samples and the generator *G* is frozen.\n",
    "\n",
    "The generator *G* is updated by minimizing the following function\n",
    "$$\\nabla_{\\theta_g} \\frac{1}{m}\\sum_{i=1}^m \\left[log\\:\\left(1-D\\left(G\\left(z^{(i)}\\right)\\right)\\right)\\right]$$\n",
    "where the discriminator *D* is frozen. Notice the difference between the two losses. The discriminator should be able to classify real samples and fake samples, where the generator is only concerned about trying to fool the discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LATENT_DIM = 4\n",
    "\n",
    "# Define GAN model\n",
    "model = models.vanilla_gan.VanillaGAN(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    latent_shape=(MODEL_LATENT_DIM,),\n",
    "    ambient_shape=(AMBIENT_DIM,)\n",
    ")\n",
    "\n",
    "# TODO: Initialize the GAN model by passing in the loss function of the discriminator\n",
    "model.initialize(loss_fn=optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, please complete the to-do's in `vanilla_gan.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell with training loop. Complete blank parts\n",
    "model.train(\n",
    "    datasets.base.TensorDataset(data), \n",
    "    optax.adam(learning_rate=1e-3), \n",
    "    print_every=5, \n",
    "    batch_size=1000, \n",
    "    num_epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the losses is not as straightforward as you would think. Since the generator and discriminator are adversaries, their loss cannot decrease simultaneously. Usually when GANs are trained we expect the discriminator loss to decrease and the generator loss to oscillate, but they are not particularly indicative of whether training was a success. To determine this, let's visualize samples from the distribution the GAN has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_distribution = model.create_distribution()\n",
    "random_noise = random.uniform(random.PRNGKey(0), (len(data), MODEL_LATENT_DIM), minval=-1, maxval=1)\n",
    "fake_samples = gan_distribution.draw_samples(random_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'x': jnp.concatenate((data[:, 0], fake_samples[:, 0]), axis=0),\n",
    "      'y': jnp.concatenate((data[:, 1], fake_samples[:, 1]), axis=0),\n",
    "      'labels': jnp.concatenate((jnp.ones((data.shape[0],)), jnp.zeros((fake_samples.shape[0],))), axis=0)}\n",
    "\n",
    "px.scatter(df, x='x', y='y', color='labels', labels={\"0\": \"Fake\", \"1\": \"True\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did the GAN manage to learn the underlying data distribution? Do you observe anything odd? Why, why not?**\n",
    "\n",
    "Looking at the figure above, you might have noticed that the generator only generates points in the blob located in the lower right corner. When training GANs, a common challenge is non-convergence and mode collapse. Mode collapse occurs as the generator converges to a local minima, where the generator only rotates through few different outputs that fool the discriminator. If the generator tries to diverge from these outputs, the discriminator notices and classifies them as fake i.e. not coming from the true data distribution. Therefore, the generator cannot explore and learn to generator data from the blob in the top left corner, as doing so would increase the loss of the generator. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cs182project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fec1743f822f2dd43bfb656bddbefe137b7c1918b0f71f6477d5e31a257d081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
