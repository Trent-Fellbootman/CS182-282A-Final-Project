{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Generative Adversarial Networks (GANs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from library.utils.numerical_checking import NumericalCheckingRecord\n",
    "from library.utils.helper_functions import check_model_forward\n",
    "from flax import linen as nn\n",
    "from jax import random, numpy as jnp\n",
    "import optax\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The generator and discriminator are both neural networks, whose architecture greatly depends on the task at hand. The most popular domain for GANs is on images, which means that ResNets and convolutions are part of the generator and discriminator architecture. Due to the computational complexity of using images and thereby training deep models, this homework will consider a points dataset, where data points are generated according a multivariate distribution with noise. As a result, the neural networks become shallower, and their architecture will deviate from what is used in the original CycleGAN paper. \n",
    "\n",
    "The points dataset in this homework is similar to the cluster dataset used in the [GAN Lab]( https://poloclub.github.io/ganlab/). GAN Lab also provides a great visualization while training a GAN. In essence, we are interested in training a GAN that can output data points within the two clusters seen in the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.utils.make_blobs(n_samples=1000, min_sigma=0, max_sigma=0.1)\n",
    "data = dataset.get_tensors()\n",
    "fig = make_subplots(rows=1, cols=1, subplot_titles=[\"Real samples\"])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data[:, 0], y=data[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real'), row=1, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (B): Define discriminator and generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, consider the discriminator and generator as multi-layer perceptrons with ReLU activations between each layer. The number of layers and neurons is a design choice, but it is important to consider the differing dimensions of the final layer of both networks. The generator should generate output of similar dimension as the data that describes the distribution you wish to learn. The discriminator should generate a single logit, indicating whether it believes some given input is from the true distribution or the generated one.\n",
    "\n",
    "Using the above, define the ouput dimensions of the discriminator and generator below. The rest of the architecture has already been given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambient dimension, discriminator and generator\n",
    "AMBIENT_DIM = 2\n",
    "\n",
    "######################################################################\n",
    "# TODO: Define the output dimensions of Generator and Discriminator  #\n",
    "# HINT: You can use AMBIENT_DIM                                      #\n",
    "######################################################################\n",
    "\n",
    "discriminator = nn.Sequential([\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(16),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(), # TODO: Complete output dimension\n",
    "])\n",
    "\n",
    "generator = nn.Sequential([\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(8),\n",
    "    nn.relu,\n",
    "    nn.Dense(4),\n",
    "    nn.relu,\n",
    "    nn.Dense(), # TODO: Complete output dimension\n",
    "])\n",
    "######################################################################\n",
    "#               END OF YOUR CODE                                     #\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation check\n",
    "key = random.PRNGKey(0) # DO NOT CHANGE\n",
    "test_discriminator_input = random.normal(key, data.shape)\n",
    "test_generator_input = random.normal(key, (1,4))\n",
    "d_out = check_model_forward(discriminator, test_discriminator_input)\n",
    "g_out = check_model_forward(generator, test_generator_input)\n",
    "\n",
    "d_expected_out = NumericalCheckingRecord.load(\"checks/gan_discriminator_check\")\n",
    "g_expected_out = NumericalCheckingRecord.load(\"checks/gan_generator_check\")\n",
    "\n",
    "assert d_expected_out.data.shape == d_out.shape, \"Output dimensions does not match expected\"\n",
    "assert g_expected_out.data.shape == g_out.shape, \"Output dimensions does not match expected\"\n",
    "assert d_expected_out.check(d_out), \"Output does not match the expected. Remember to output logits\"\n",
    "assert g_expected_out.check(g_out), \"Output does not match the expected. Remember to output logits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (C): GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training can commence, you need to implement the loss functions to train the generator and discriminator. \n",
    "The discriminator is trained to maximize the probability of assigning the correct labels for both real samples and generated samples, and train the generator to fool the discriminator simultaneously. In other words, generator and discriminator are playing a minmax game with the value function $V(G, D)$:\n",
    "\n",
    "$$\\underset{G}{min}\\:\\underset{D}{max} \\:V(G, D) = \\mathbb {E}_{x\\sim p_{data}(x)}[log\\:D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[log(1 - D(G(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective can decomposed into an individual loss function for the discriminator and generator.\n",
    "\n",
    "The discriminator *D* tries to maximize the following function, which is largely similar to binary cross entropy\n",
    "$$\\nabla_{\\theta_d} \\frac{1}{m}\\sum_{i=1}^m \\left[log\\: D\\left(\\mathbb{x}^{(i)}\\right)+log \\: \\left(1-D\\left(G\\left(z^{(i)}\\right)\\right)\\right)\\right]$$\n",
    "where *m* is the number of examples in the minibatch, *x* is the true samples, *z* is a vector of noise samples and the generator *G* is frozen.\n",
    "\n",
    "The generator *G* is updated by minimizing the following function\n",
    "$$\\nabla_{\\theta_g} \\frac{1}{m}\\sum_{i=1}^m \\left[log\\:\\left(1-D\\left(G\\left(z^{(i)}\\right)\\right)\\right)\\right]$$\n",
    "where the discriminator *D* is frozen. Notice the difference between the two losses. The discriminator should be able to classify real samples and fake samples, where the generator is only concerned about trying to fool the discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LATENT_DIM = 4\n",
    "\n",
    "# Define GAN model\n",
    "model = models.vanilla_gan.VanillaGAN(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    latent_shape=(MODEL_LATENT_DIM,),\n",
    "    ambient_shape=(AMBIENT_DIM,)\n",
    ")\n",
    "\n",
    "model.initialize(loss_fn=optax.sigmoid_binary_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, please complete the to-do's in `vanilla_gan.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell with training loop. Complete blank parts\n",
    "model.train(\n",
    "    datasets.base.TensorDataset(data), \n",
    "    optax.adam(learning_rate=1e-3), \n",
    "    print_every=5, \n",
    "    batch_size=1000, \n",
    "    num_epochs=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the losses is not as straightforward as you would think. Since the generator and discriminator are adversaries, their loss cannot decrease simultaneously. Usually when GANs are trained we expect the discriminator loss to decrease and the generator loss to oscillate, but they are not particularly indicative of whether training was a success. To determine this, let's visualize samples from the distribution the GAN has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_distribution = model.create_distribution()\n",
    "random_noise = random.uniform(random.PRNGKey(0), (len(data), MODEL_LATENT_DIM), minval=-1, maxval=1)\n",
    "fake_samples = gan_distribution.draw_samples(random_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation check\n",
    "expected_fake_samples = NumericalCheckingRecord.load('checks/gan_output_check')\n",
    "assert expected_fake_samples.check(fake_samples), \"Output does not match expected. Please revisit your implementation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=1, subplot_titles=[\"Real and fake samples from trained generator\"])\n",
    "fig.add_trace(go.Scatter(x=data[:, 0], y=data[:, 1], mode='markers', marker=dict(color=\"blue\"),name='Real'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=fake_samples[:, 0], y=fake_samples[:, 1], mode='markers', marker=dict(color=\"red\"),name='Fake'), row=1, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Did the GAN manage to learn the underlying data distribution? Do you observe anything odd? Why, why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers in the written portion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('cs182project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fec1743f822f2dd43bfb656bddbefe137b7c1918b0f71f6477d5e31a257d081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
